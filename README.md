# Identify Customer Segments

A complete, endâ€‘toâ€‘end workflow for discovering meaningful clusters in a generalâ€‘population demographics dataset using modern Python **(â‰¥â€¯3.11)**, scikitâ€‘learn, and complementary dataâ€‘science libraries. The project walks through data cleaning, feature engineering, dimensionality reduction, clustering, evaluation, and interpretation wrapping everything in reproducible notebooks and scripts.

---

## âœ¨ Project Highlights

* **Clean, modular codeÂ base** with clear separation between data, notebooks, and reusable pipelines.
* **Scalable preprocessing**: automated missingâ€‘value handling and oneâ€‘hot/ordinal encoding that adapts to arbitrary tabular inputs.
* **Dimensionality reduction** via PCA to tame highâ€‘dimensional data while preserving >â€¯90â€¯% of variance.
* **Cluster discovery** with Kâ€‘Means (elbow & silhouette diagnostics) plus optional Gaussian Mixture & HDBSCAN extensions.
* **Rich visualizations**â€”elbow curve, silhouette plot, cluster heatmaps, and interactive embeddings.
* **Jupyter + CLI**: run the whole analysis in notebooks or headless via the command line.
* **Reproducible environment**: lockfile for exact package versions and Makefile targets for common tasks.

---

## ğŸ”§ Requirements

| Package      | Tested Version | Notes                               |
| ------------ | -------------- | ----------------------------------- |
| Python       | **3.11**       | â‰¥â€¯3.11 recommended; 3.10 also works |
| numpy        | 1.26.x         |                                     |
| pandas       | 2.2.x          |                                     |
| scikitâ€‘learn | 1.5.x          |                                     |
| matplotlib   | 3.9.x          |                                     |
| seaborn      | 0.13.x         | optional, for nicer charts          |
| jupyterlab   | 4.x            | for notebook workflows              |
| joblib       | 1.5.x          | model persistence                   |
| tqdm         | 4.x            | progress bars                       |

Install with **conda** (recommended):

```bash
conda env create -f environment.yml
conda activate customerâ€‘segments
```

â€”or with **pip**:

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸš€ Quickstart

1. **Clone** the repo

   ```bash
   git clone https:github.com/amroadnan22/udacity_Clustering_Customer_Segments.git
   cd customerâ€‘segments
   ```
2. **Install** dependencies (see above).
3. **Add data**: place your raw CSV/Parquet files in `data/raw/`. Example dataset schema is documented in `notebooks/01_data_cleaning.ipynb`.
4. **Run the full pipeline** with Make:

   ```bash
   make all        # clean â†’ engineer â†’ pca â†’ cluster â†’ report
   ```

   or open the notebooks sequentially in JupyterLab for an interactive walkthrough.
5. **Inspect results** in `results/` and the final notebook `04_results.ipynb`.

---

## ğŸ§ª Methodology & Code Walkthrough

### 1Â â–¸Â Data Cleaning (`data_prep.py` / `01_data_cleaning.ipynb`)

* **Missing values**: numeric â†’ median impute, categorical â†’ new `"Unknown"` category.
* **Outlier guardrails**: winsorization at the 1st/99th percentiles.
* **Train/test split** by stratified sampling on key demographics.

### 2Â â–¸Â Feature Engineering (`features.py`)

* Oneâ€‘hot encode nominal vars (dropâ€‘first to avoid collinearity).
* Ordinalâ€‘encode ordered categories.
* Standardâ€‘scale all numeric columns.

### 3Â â–¸Â Dimensionality Reduction (`dimensionality.py`)

* Fit PCA on the training set, retaining components that explain **â‰¥â€¯90â€¯% variance** (typically 40â€“50 PCs).
* Save the fitted transformer to `models/pca.joblib`.

### 4Â â–¸Â Clustering (`clustering.py` / `03_clustering.ipynb`)

* Iterate **kâ€¯=â€¯2â€¦30**; record *inertia* and *silhouette* scores.
* Select optimal **k** via elbow + peak silhouette (defaults to k=6).
* Persist the final `KMeans` model to `models/kmeans_k6.joblib`.

### 5Â â–¸Â Evaluation & Interpretation (`04_results.ipynb`)

* Silhouette plot to visualise cohesion vs. separation.
* PCA 2â€‘D scatter coloured by cluster label.
* Cluster profiles: feature means Â± std vs. population means.

---

## ğŸ“Š Example Results

*Silhouette Score:* **0.42** @ kâ€¯=â€¯6
*Top distinguishing features:* `Age`, `Income`, `Urbanicity`, `FamilyStatus`.

Full discussion and figures are embedded in the results notebook.

---

## ğŸ—ºï¸ Roadmap / Future Work

* AutoML sweep for alternative clustering algorithms (GMM, HDBSCAN, spectral).
* SHAP or permutation importance for deeper feature impact analysis.
* Deploy as a **FastAPI** microservice that returns cluster IDs for incoming records.

---

## ğŸ¤ Contributing

1. Fork the repo & create a feature branch.
2. Run `preâ€‘commit install` to autoâ€‘format with *ruff* & *black*.
3. Open a pull request describing your changes.

Please adhere to the existing code style and write unit tests for new logic.

---

## ğŸ“„ License

Distributed under the **MIT License**. See `LICENSE` for details.

---

## ğŸ™‹â€â™‚ï¸ Contact

**AmroÂ AdnanÂ Badran**
\[amroadnanb@gmail.com](mailto:amroadnanb@gmail.com)
Questions? Ideas? Feel free to open an issue or email me directly.
